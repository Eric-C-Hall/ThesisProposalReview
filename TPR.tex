\documentclass{article}

\title{Thesis Proposal Review}
\author{Eric Hall}

\begin{document}

\maketitle

\section{Thesis Proposal}

\subsection{Purpose of Thesis}

The thesis will be to produce a formally verified implementation of the Viterbi algorithm.

To create a principled, foundational implementation of error correcting code techniques. Existing implementations aren't as foundational, or ...

Whereas other approaches take an approach where they formally prove properties in the programming language itself, using interactive-theorem prover based methods have the additional improvements of increased fidelity of proof techniques, as you do not have to rely on an automated system to do all the proof for you. Furthermore, the automated system often takes a long time to perform its proofs. Looking at things from a human perspective and proving thnigs manually gives you the ability to direct the proof and use human knowledge explicitly to prove known results.

\subsection{What is the Viterbi algorithm}

The Viterbi algorithm is a 

\subsection{Important question: Why Viterbi and not something else? What advantages does Viterbi have?}

\subsection{Important question: What research has been done so far?}

\subsection{}

\subsection{Starshot Resilient multi-mission space}

This project aligns closely with defence innovation network StarShot priority areas.

In particular, it would be useful in ``secure and resilient communications delivered from space for a highly networked force.''
%
%\subsection {Copy pasted from first research proposal}
%
%The purpose of this project is to use an interactive theorem prover to formally prove foundational mathematical theorems in the area of algebra, including such areas as group theory, field theory, and Galois theory. Future researchers will be able to build upon these proofs, improving our confidence that our mathematical theories are correct.
%
%Theorem proving has traditionally been done by hand, but this is time-consuming and prone to error. While humans are very good at understanding things on a conceptual level, they have a tendency to make silly mistakes when performing simple, boring, repetitive tasks, and the conceptual understanding they have of the topic has a tendency to obscure unjustified leaps in logic that appear “intuitive” to the human. Furthermore, the level of confidence that a third party has on the validity of a proof depends largely on how much they trust that the person performing the proof has done a good job at verifying their proof is correct. If they do not have such trust, then they must perform the boring, repetitive work of verifying the proof for themselves.
%
%The simple, boring and repetitive nature of checking the validity of a proof makes it a prime target for automation by computers, which excel at simple and repetitive tasks. By using a computer to perform the brute work of verifying the correctness of the proof, the user is able to prove theorems to an impeccably high standard of formality, without the arduous process of checking that each stage of the proof is formally correct. It also allows a third party to have a high level of trust in the validity of the proven theorem without having to check the proof themselves, so long as they trust that the proof-checking software has been programmed correctly.
%
%Furthermore, some theorem proving can be performed automatically by computers. This is best suited to simple and tedious derivations that need to be performed but do not require a high degree of understanding and are simply routine proofs. Using an automated theorem prover to make these derivations will take some of the load off the human prover, freeing the human from being bogged down in the minutiae of formal proof. Beyond that, the automated theorem provers may also have the ability to perform non-trivial proofs without the expenditure of human effort.
%
%Automated theorem provers have weaknesses, though. The place where humans really shine is in having a deep understanding of the meaning behind the theorems. Whereas the computer is just manipulating symbols, the human will typically have a better conceptual understanding of what these symbols mean and why they are important, and how they relate to each other. While work is ongoing in order to improve the ability of computers to attain this kind of understanding, this is still one of their weaknesses and humans will have an advantage in solving complex problems where a general understanding of the problem to be solved is useful.
%
%In this way, the human and computer solver cover each other’s weaknesses: the human excels at understanding the problem, whereas the automated theorem prover is very good at performing the grunt work.
%
%One major application area of interactive theorem proving is in proving the correctness of vital software in scenarios in which no bugs are tolerable. Examples of such application areas include cryptography, vote-counting software, medical software, software for controlling spacecraft or other vehicles, software for controlling industrial machinery such as nuclear power plants, etc. In such scenarios where any bug could have a catastrophic effect, it is important to be confident that your systems work as intended. By using an interactive theorem prover to prove that the output of the relevant code is correct, we may achieve the necessary confidence that our code is safe.
%
%Interactive theorem proving is also useful for improving our confidence in our understanding of mathematics and adjacent fields in general. Mathematics underpins much of the modern world, and to be confident in the effectiveness of many real-world systems, it is important to be confident in the underlying mathematics. The process of formalizing our mathematical theories is likely to expose flaws in our current understanding of mathematics, especially in the less well-trodden areas where the proofs have not been exposed to the level of scrutiny that is typical of more widely studied areas of mathematics.
%
%My specific project aims to be a foundational building block in achieving these goals. I aim to produce a computer-assisted formalization of commonly used mathematical theorems, so that future researchers can build on top of this to produce computer-formalized proofs of theorems that are important in their own application areas. This could be used by future researchers both in domain-specific applications and in simply solidifying our understanding of mathematics in general. 
%
%The specific area I would like to work in is algebra: in particular, areas such as group theory, field theory, and Galois theory. These are fundamental areas of study of vital importance to a large number of other disciplines, which is why I chose them as a topic of study.
%
%As of right now, I am still in the beginning stages of literature review to identify pre-existing research on the topic. There does seem to be at least some relevant literature already on the topic of the formalization of Galois theory (see [1] and [2]). Although research has been done in this area before, there is still likely room to perform more research, as there are many useful theorems and only some will have been formalized. In addition, I expect that there are many closely related fields of algebra which have not yet had formalization work applied to them.
%
%Novel ideas will be required in order to formalize the mathematics, because the theorems have not been proven to the required standard of rigor necessary. The existing proofs will likely be in a human-readable form rather than in a formal, computer-readable form, and as a result they will likely be missing key details necessary to make the proofs work in an interactive theorem prover setting.
%
%References:
%
%[1]    T. Browning and P. Lutz. “Formalizing Galois Theory”. Experimental Mathematics, vol. 31, pp 413-424, 2021
%
%[2]    N. Curiel. “Formalizing Galois Theory I: Automorphism Groups of Fields”. Master’s Thesis, California State University San Marcos, MSc, 2011
%
%[3]    M. Norrish. “Mechanised Computability Theory”. International Conference on Interactive Theorem Proving, pp 297-311, 2011
%
%[4]    R. Affeldt, J. Garrigue and T. Saikawa. “Formalization of Reed-Solomon codes and progress report on formalization of LDPC codes”. International Symposium on Information Theory and Its Applications, pp 532-536, 2016
%
%https://ieeexplore.ieee.org/document/7840481 Formalization of Reed-Solomon codes and progress report on formalization of LDPC codes
%
%\subsection{Copy pasted from second research proposal}
%
%The purpose of this project is to use an interactive theorem prover to produce a formally verified implementation of the Viterbi algorithm. This would also involve the formalization of underlying mathematical theorems which could be of general use.
%
%When reading data from a noisy channel, errors are likely to occur. If we provide redundant information over this channel, then when an error occurs, it is possible to use the redundant information to correct the error with high probability [1]. Encodings of data that allow for this error-correcting property are called error-correcting codes.
%
%The Viterbi algorithm is an algorithm that is used to decode a certain class of error-correcting codes [2].
%
%Interactive theorem proving is a technique used to formally prove that a system works correctly with respect to a specification [3]. As long as there is trust that the theorem prover and the specification are both correct, this provides an 100% guarantee that the algorithm behaves correctly. This is useful in situations where failure could be catastrophic and thus we need extremely reliable software, for example in pacemakers or self-driving cars.
%
%It would be useful to have a formally verified implementation of the Viterbi algorithm, because the algorithm may be used in systems that have a requirement of high reliability. It is extremely common to need to read information from a potentially noisy channel. Common examples include reading from a wireless connection [4] or even from a hard drive [5]. Most real-world systems would have a need to do these kinds of basic tasks, thus, most real-world systems have a need for error-correcting codes to some degree. This includes those systems that would benefit from the correctness guarantees provided by formal proof. Thus, the improved reliability of having a formally verified implementation of the Viterbi algorithm would be useful in practice in real-world systems with low fault tolerance.
%
%Formalization of other error correcting codes has been performed in the past. For example, the correctness of Reed-Solomon codes has been formally proved using interactive theorem provers [6]. Also, the correctness of the Viterbi algorithm has been proven using handwritten methods [2], although handwritten methods are by nature less reliable than computer-verified methods. However, in my preliminary review of the literature, I have not encountered a computer verified formalization of the Viterbi algorithm. Thus, it should satisfy the novelty requirements of a PhD. 
%
%The formal proving process is not merely a mechanical implementation process. Previous proofs of the correctness of the Viterbi algorithm were written to satisfy a human reviewer, not to satisfy a formal computer verifier. Therefore, they almost certainly handwave parts of the proof, and have gaps in them which they fail to prove to a sufficient standard of rigor. In order to fill in the gaps in the proof, it will be necessary to come up with new ideas, rather than merely implementing old ones. Thus, the project also has the element of research that is expected of a PhD (rather than being pure development work).
%
%The formalization will require a significant amount of work, because proving something to the requisite standard of rigor necessary for formal proof requires it to be proven to a high level of detail. Furthermore, there is ample room for extending the project in case it takes less work than expected. In 2016, R Affeldt et al said that formal verification of error-correcting codes is understudied [6], and based on my preliminary review of the literature, I agree that there is a significant amount of room for expansion. Thus, even in the case where the production of a formally verified Viterbi algorithm takes less work than expected, it should be possible to extend the project by performing more work, for example, by proving the correctness of other error correcting code algorithms. Therefore, it should be possible to perform sufficient work in this area to be appropriate for a PhD.
%
%On the other hand, the amount of work required can be realistically achieved within a PhD project. Interactive theorem provers are a proven technology that have been used to produce formally verified implementations of several algorithms, for example a formally verified OS kernel [7] and a formally verified compiler of a pure programming language [8]. This shows that the techniques necessary to produce a formally verified implementation of even complex algorithms is available. On the one hand, the formal implementations of these algorithms involved several authors, but on the other hand they also were implementations of relatively complicated systems, so in my estimation the amount of work required to produce a formally verified implementation of the Viterbi algorithm should be appropriate for a PhD.
%
%To evaluate my progress mid-way through the PhD, it should be possible to look at the proofs and specifications that have been produced so far. The project will have several stages which will need to be completed before the project as a whole can be completed. For example, we will need to create a specification for the Viterbi algorithm, we will need to prove theorems describing the mathematical background that the Viterbi algorithm is based upon, and we will need to prove the correctness of individual parts of the Viterbi algorithm. Then in order to evaluate how much progress I have made in my PhD, I should be able to look at how many proofs/specifications have been written so far. This would allow us to have a sense of whether or not progress was being made on the project, providing a greater guarantee that the PhD program would be completed successfully and on time.
%
%In summary, my proposal is to use an interactive theorem prover to produce a formally verified implementation of the Viterbi algorithm. This would be useful because error-correcting codes are necessary in a wide variety of applications, including those in which it is necessary to have a high level of confidence in the correctness of the software. This project would be feasible, as similar projects have been completed before, and this project would take a significant amount of work, as a high level of detail would be necessary to provide a sufficiently rigorous formal proof. It would furthermore be possible to evaluate the progression of the project by evaluating the work that has been completed so far, which could include a specification for the Viterbi algorithm or the proof of sub-theorems necessary in the correctness proof of the overall algorithm.
%
%References:
%
%[1]    J. Baylis, Error Correcting Codes: A Mathematical Introduction. Boca Raton: Chapman \& Hall, 1998.
%
%[2]    A. Viterbi, “Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm,” IEEE Transactions on Information Theory, vol. 13, pp. 260-269, Apr 1967.
%
%[3]    Y. Bertot, P. Casteran, Interactive Theorem Proving and Program Development: Coq’Art: The Calculus of Inductive Constructions. Heidelberg: Springer-Verlag, 2004.
%
%[4]    M. C. Vuran, I. F. Akyildiz, “Error Control in Wireless Sensor Networks: A Cross Layer Analysis”, IEEE/ACM Transactions on Networking, vol. 17, pp. 1186-1199, Aug 2009
%
%[5]    D. Patterson, G. Gibson, R. H. Katz, “A Case for Redundant Arrays of Inexpensive Disks (RAID),” Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data, pp. 109-116, Jun 1988
%
%[6]    R. Affeldt, J. Garrigue and T. Saikawa, “Formalization of Reed-Solomon codes and progress report on formalization of LDPC codes,” International Symposium on Information Theory and Its Applications, pp. 532-536, Oct 2016.
%
%[7]    G. Klein, K. Elphinstone, G. Heiser, J. Andronivk, D. Cock, P. Derrin, D. Elkaduwe, K. Engelhardt, R. Kolanski, M. Norrish, T. Sewell, H. Tuch, S. Winwood, “seL4: Formal Verification of an OS Kernel,” Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles, pp. 207-220, Oct 2009.
%
%[8]    R. Kumar, M. O. Myreen, M. Norrish, S. Owens, “CakeML: a verified implementation of ML,” ACM Sigplan Notices, vol. 49, pp. 179-191, Jan 2014.
%
\section{Literature Review}

\subsection{Viterbi related}

\subsubsection{Wikipedia}

The Viterbi algorithm is an algorithm which allows you to determine the most likely sequence of states in a hidden Markov model that lead to a certain sequence of observations.

Note that this sequence of states may not necessarily have a high probability, but it is the most likely sequence of states out of all possible sequences of states.

This algorithm is used in decoding convolutional codes. For example, (direct quote here):  ``CDMA and GSM digital cellular, dial-up modems, satellite, deep-space communications, and 802.11 wireless LANs.''

(direct quote) ``It is now also commonly used in speech recognition, speech synthesis, diarization,[1] keyword spotting, computational linguistics, and bioinformatics''

It is a dynamic programming algorithm.

Let $P_{s,t}$ denote the probability of the maximum probability path ending in the state $s$ at the time $t$.

At $t = 0$, we know that $P_{s,t}$ is equal to the prior probability of being in that state multiplied by the probability of seeing the first observation if you are in that state.

At each time beyond that, $P_{s,t}$ is equal to the maximum over all states $s'$ of $P_{s', t-1}$ multiplied by the transition probability between $s'$ and $s$, multiplied by the probability of seeing the observation that was observed at time $t$ if we are in state $s$.

Also make sure to keep track of the transitions that were used to obtain each maximum probability path.

Thus we can use dynamic programming techniques to first calculate all the probabilities at time 0, then calculate all the probabilities at time 1, then calculate all the probabilities at time 2, etc. This will avoid having to re-calculate the probabilities at time 0 multiple times when, for example, finding the maximum probability paths to several different states at a time far in the future. In particular, this will not need to be calculated once for every possible path it is possible to take, but rather, it will only need to be calculated once.

\subsubsection{(high importance) Viterbi's original paper: Error bounds for convolutional codes and an asymptotically optimum decoding algorithm }

\subsubsection{Viterbi algorithm for error correction}

\subsubsection{(unknown quality, google search) The Viterbi Algorithm, by G David Forney, JR}

\subsubsection{Wikipedia Convolutional Codes}

Viterbi is used for small values of $k$. Longer constraint length codes are often decoded using sequential decoding algorithms e.g. the Fano algorithm.

\subsubsection{Turbo codes}

More modern approach than Viterbi that approach theoretical limits imposed by Shannon's theorem.

% --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\subsection{Formal ECC Verification}

\subsubsection{A Library for Formalization of Linear Error-Correcting Codes 2020, 8 citations}

Systematic formalization of error correcting codes in the Coq proof-assistent.

\subsubsection{Robust error detection in communication and computational channels, 2007, 72 citations}

\subsubsection{Formal verification of error correcting circuits using computational algebraic geometry, 2012}

\subsubsection{Formalization of Reed-Solomon codes and progress report on formalization of LDPC codes, 2016}

\subsubsection{Formalization of Error-correcting Codes: from Hamming to Modern Coding Theory, 2015, 23 citations}

\subsubsection{Verification of Galois field based circuits by formal reasoning based on computational algebraic geometry}

\subsubsection{Error correction code algorithm and implementation verification using symbolic representations}

\subsubsection{UVM-based verification of ECC module for flash memories}

\subsubsection{Verified erasure correction in Coq with MathComp and VST}

\subsubsection{Formal verification of ECCs for memories using ACL2}

\subsubsection{Formalization of coding theory using lean}

\subsubsection{A Coq formalization of information theory and linear error-correcting codes.}

\subsubsection{Formalization of theorems about stopping sets and the decoding performance of LDPC codes}

\subsubsection{Formal verification of sequential galois field arithmetic circuits using algebraic geometry}

\subsubsection{Formal verification of ECCs for memories using ACL2}

\subsubsection{Formalization of insertion/deletion codes and the Levenshtein metric in lean}

See also: Coding for Insertions and Deletions, quote: in fact, our understanding of insertion-deletion codes significantly lags behind our thorough understanding of error correcting codes (ECCs)

\subsubsection{A Pragmatic Approach to Extending
Provers by Computer Algebra—with Applications to Coding Theory}

\subsubsection{Coding Theory of Permutations/Multipermutations and in the LEAN Theorem Prover}

\subsubsection{(unkown quality, google search) Pragmatic Formal Verification of Sequential Error
Detection and Correction Codes (ECCs) used in
Safety-Critical Design (Aman Kumar) 2024}

(Seems relevant, more review necessary)

Formal verification

Mentions Hamming codes, Hsaio codes, Reed-Solomon codes and Bose-Chandhuri-Hocquenghem codes

(direct quote) ``The ECC used in this work is a combinatorial Quad Bit Error Detection, Triple Bit Error Correction (QEDTEC) ECC''.

Prior approach was simulation-based, testing style approach

New approach use mathematical analysis and proof methods.

Keyword property is used in the formal prover

What formal prover is used?

What specific ECC is used? Description is ambiguous

Formal verifier takes design and desired properties as inputs.

(systemverilog assertions)

Seems to use SystemVerilog

For larger designs, can take a long time to verfiy.

Algorithm consists of Syndrome generator, error detection unit and error correction unit.

My algorithm works in a completely different way using completely different tooling. It uses more authomated kinds of proof techniques

\subsubsection{Formal Verification by The Book: Error Detection and Correction Codes 2020}

UVM-based in previous work.

Seems to be in a similar field to Aman Kumar's work, and not related to interactive theorem provers. Therefore not relevant to my work.

\subsubsection{Verification of an error correcting code by abstract interpretation, by Charles Hymans 2005}

Validated Reed-Solomon

VHDL (VHSIC Hardware Description Language)

Also uses formally verified programming, not interactive theorem provers.

Uses abstract interpretation: unrelated to my work.

% --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\subsection{Error-Correcting codes theory}

\subsubsection{Error correcting codes: A  mathematical introduction}

\paragraph{Linear codes}

0 is in all linear codes

linear code minimum distance is given by minimum weight

linear codes correction depends only on error vector, not on  specific code

For each position in a linear code, it is either always 0, or every option for this position appears equally often.

linear codes agree with basic linear algebra

Generator matrix: Take a basis for the code space, and create a matrix $G$ in which each row is an element of the basis. Then for each message m, the codeword is described by $m G$

Particularly convenient if the generator matrix has the identity if you cut off the last few columns.

Hamming codes are linear, with identity at beginning.

Consider the codes in the codespace (a.k.a. those generated by the generator matrix). Write these down with 0 at the beginning. Pick a code not in the codespace with the lowest possible weight. Consider the coset by adding this code to each of the codes in the codespace. Pick another code, which is neither in the codespace nor the new coset, with the lowest possible weight, and consider its coset. Continue this process until each code is in some coset. Write these cosets in an array, where each row contains a coset (including the coset where nothing is added to the codespace), and the first element in the first column is 0, and elements in the same column are derived by adding the corresponding code to the original code. The result is called the "Slepian array"

You can decode a word using the Slepian array by finding it in the array, and returning the corresponding element in the first row. This method is equivalent to nearest neighbour decoding.

The coset leader is the number which is added to the original element of the codespace to create the coset. They are the elements in the first column, because 0 + anything is the original thing.

Although different elements can be chosen to generate the cosets, the set of cosets will always be the same if ordering is ignored. That is, the cosets can be swapped around, and elements inside the cosets can be swapped around, but corresponding cosets will contain corresponding elements, and we will always have corresponding cosets no matter how we choose the elements to generate them.

Parity check matrix: independent rows, codespace is the null space of the matrirx

Every linear code has a parity check matrix.

Dual code: If $C$ is an $[n,k]$ code over $Z_p$, then $C^\perp$ is an $[n, n - k]$ code

Matrix is parity check matrix for $C$ if and only if it is a generator matrix for $C^\perp$

Note: an $[n,k]$ code has $n$ dimensions in the encoded form and $k$ dimensions in the decoded form.

Syndrome decoding: allows you to avoid storing whole Slepian array for decoding, and avoid having to search through the whole array to find the element to decode.

Let $H$ be parity check matrix, $v$ any word. Then syndrome of $v$ is $vH^T$

Two elements are in the same coset iff they have the same syndrome.

If $C$ is $[n,k]$ code and $H$ is any parity check matrix for it, $e$ is error pattern associated with received word $r$, then syndrome of $r$ is $\left(\sum\limits_{i=1}^n e_i h_i\right)^T$, where $h_i$ is the $i$th column of $H$.

Special case for binary codes: look into more.

In syndrome decoding, just need coset leaders, syndromes, and parity check matrix

Minimum distance of linear code $C$ is the size of the smallest dependent set of columns of $H$.

\paragraph{Hamming Family}

Hamming codes: Ham$(r, q)$ is the set of those linear $[n, k]$ codes whose parity check matrices have $r$ rows and $n$ columns, where $n$ is the largest possible number of columns that have no pair of columns dependent.

All codes in Ham$(r, q)$ have length $n = \frac{q^r - 1}{q - 1}$

All codes in Ham$(r,q)$ are linearly equivalent to each other.

All Hamming codes have a minimum distance of 3

All Hamming codes are perfect

(todo what does this mean) In any linear code, the distribution of codeword weights is indentical to the distance distribution.

The dual of any Hamming code is called a simplex code. 	

All simplex codes are equideistant codes.

todo: continue from theorem 6.7

\subsubsection{(found via google search) A course in error-correcting codes, by J Justesen, T Høholdt }

Includes convolutional codes info

Decoding Reed Solomon

Decoding BCH

Algebraic geometry

\subsubsection{(cited by Aman Kumar, book) Error Detection and Correction Codes by Diego L. Gonzalez}

Moderate relevance. Information on ECCs

\subsubsection{(googled convolutional block codes) http://web.mit.edu/6.02/www/f2010/handouts/lectures/L8.pdf}

\subsubsection{(found google search different book) A Course in Algebraic Error-Correcting Codes}

Shannon's Theorem, Finite Fields, Block Codes, Linear Codes, Cyclic codes, Maximum Distance Separable Codes, Alternant and Algebraic Geometric Codes, Low Density Parity Check Codes, Reed-Muller and Kerdock Codes, p-Adic codes

\subsubsection{Hamming, R.W.: Error detecting and error correcting codes.}

\subsubsection{The Theory of Information and Coding, Encyclopedia of Mathematics and its Applica-
tions}

Information theory: entropy, discrete memoryless channels, discrete memoryless sources, the gaussian channel, the source-channel coding theorem

Coding theory: linear codes, cyclic codes, BCH, Reed-Solomon, convolutional codes, variable-length source coding

\subsubsection{Polynomial codes over certain finite fields}

\subsubsection{Modern Coding Theory. (probably intend the one by T. Richardson, R. Urbanke, 2008)}

Factor graphs, binary erasure channel, binary memoryless symmetric channels, general channels, turbo codes, general ensembles, expander codes and flipping algorithm,

% --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\subsection{Other theory}

Soft errors are non-repeatable, temporary errors caused by external interference.

Hard errors are caused by a repeatable problem, e.g. a problem in the circuit or algorithm.

% --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\subsection{Other formal verification}

\subsubsection{Verification, Model Checking, and Abstract Interpretation conference}

\subsubsection{Interactive Theorem Proving conference}

\subsubsection{(cited by Charles Hymans) A static analyzer for large safety-critical software}


\subsubsection{Vale: Verifying {High-Performance} Cryptographic Assembly Code}

\subsubsection{(cited by Aman Kumar) Formal Verification, An Essential Toolkit for Modern VLSI Design} 

\subsubsection{Formalizing 100 theorems}

\subsubsection{Bel-Games: A Formal Theory of Games of Incomplete Information Based on Belief Functions in the Coq Proof Assistant}

\subsubsection{Reasoning with Conditional Probabilities and Joint Distributions in Coq}

\subsubsection{An Introduction to MathComp-Analysis}

\subsubsection{Formalization of Shannon’s theorems}

\subsubsection{Solutions to IBM POWER8 verification challenges}

\subsubsection{Verification of Composite Galois Field Multipliers over GF (2\^m\^n) Using Computer Algebra Techniques}

\subsubsection{Examples of formal proofs about data compression}

\subsubsection{Classification of finite fields with applications}

\subsubsection{Certifying assembly with formal security proofs: the case of BBS}


% --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\subsection{Soft errors (what are these? Probably unrelated)}

\subsubsection{(cited by Aman Kumar, book) Soft errors in Modern Electronic Systems}

\subsubsection{(cited by Aman Kumar, book) Dependability in electronic systems - Mitigation of Hardware Failures, Soft Errors, and Electro-Magnetic Disturbances}

Probably only tangentially relevant

% --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\subsection{Low relevance}

\subsubsection{(cited by Aman Kumar) Error correcting code analysis for cache memory high reliability and performance}

Low relevance


\subsection{(cited by Aman Kumar) Formal verification by the book: Error detection and correction codes}
Signals and Communication Technology
Martin Tomlinson
Cen Jung Tjhai
Marcel A. Ambroze
Mohammed Ahmed
Mubarak Jibril
Moderate/High relevance

\subsection {(from google) Error-Correction Coding and Decoding by Martin Tomlinson, Cen Jung Tjhai, Marcel A. Ambroze, Mohammed Ahmed, Mubarak Jibril}

Lots of interesting codes, e.g. algebraic geometry codes

\subsection {Joe Hurd PhD thesis Formal verification of probabilistic
algorithms (recommended by Michael)}

The thesis is aqbout how to fomally verify probabilitic algorithms. Not specific to HOL4, although HOL4 is the theorem prover that was used.

\begin{itemize}
\item {Formally verified probabilistic algorithms example}
\item {Probability foundations}
\item {Probabilstic program modelling}
\item {Verify Miller-Rabin test}
\item {}
\item {}
\item {}
\item {}
\end{itemize}


\subsection{Formalizing Integration Theory, with an
Application to Probabilistic Algorithms
Stefan Richter 
(unkown quality, found via google because it cites Joe Hurd's thesis)}

Conclusion: unlikely to be relevant. In Isabelle. On the other hand, is an approach for formalization of Probability-relevant background.

\begin{itemize}
\item {Uses Isabelle}
\item {Lesbegue integration}
\item {Sigma algebra}
\item {Monotone convergence}
\item {Measure spaces}
\item {Real-valued random variables}
\item {Integration}
\item{Probability spaces}
\end{itemize}

\section{Formalization of Reed-Solomon codes and progress report on formalization of LDPC codes}

\section{Research activities since commencement}

\subsection {Fermat's Little Theorem}

\subsection {Simple block codes proof}

\subsubsection {Block codes theory}

\subsubsection{}	


\subsection {Extreal to real, polynomial solver.}


\section{Methodology to be Employed}

The HOL4 interactive theorem prover will be used to prove theorems.

\section{How my research will contribute to my field of study}

\section{Bibliography}


\end{document}